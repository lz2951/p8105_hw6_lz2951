---
title: "p8105_hw6_lz2951"
author: "lz2951"
date: "2023-12-01"
output: github_document
---

```{r}
library(tidyverse)
library(modelr)
library(mgcv)
library(purrr)
set.seed(1)
```

# Problem 2

## Download data

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

## Boostrap

```{r}
boostrap_results =
  weather_df |>
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy),
    glances = map(models, broom::glance)
  ) |>
  select(-strap, -models) |>
  unnest(glances) |>
  select(.id, results, r.squared) |>
  unnest(results) |>
  select(.id, r.squared, term, estimate) |>
  pivot_wider(names_from = term, values_from = estimate) |>
  select(-`(Intercept)`) |>
  mutate(logbeta = log(tmin * prcp))
```

## Plot

The distribution of ${\hat{r}}^2$:

```{r}
boostrap_results |>
  ggplot(aes(x = r.squared)) + 
  geom_density() +
  ggtitle("Density Plot Showing the Distribution of Estimated Squared r among Boostrap")
```

According to the density plot, the distribution of squared r is essentially a symmetric bell-shaped curve. The range of squared r falls between 0.87 and 0.95, with a peak around 0.92. This indicates that the predictive performance of the simple linear regression model for predicting tmax using tmin and prcp as predictors is satisfactory. The distribution of squared r is not apparently skewed, suggesting that squared r is not significantly influenced by individual outlier values.

The distribution of $log(\hat{\beta}_1*\hat{\beta}_2)$

```{r}
boostrap_results |>
  ggplot(aes(x = logbeta)) + 
  geom_density() +
  ggtitle("Density Plot Showing the Distribution of Estimated log(beta0*beta1) among Boostrap") +
  xlab("log(beta0*beta1)")
```

According to the density plot, the distribution of $log(\hat{\beta}_1*\hat{\beta}_2)$ is a left-skewed curve. The peak of $log(\hat{\beta}_1*\hat{\beta}_2)$ falls around -5.5. This obviously left-skewed distribution of $log(\hat{\beta}_1*\hat{\beta}_2)$ suggests that $log(\hat{\beta}_1*\hat{\beta}_2)$ is significantly influenced by individual extremely small values.

The 95% confidence interval for ${\hat{r}}^2$ is (`r quantile(boostrap_results$r.squared, 0.025)`, `r quantile(boostrap_results$r.squared, 0.975)`)

The 95% confidence interval for $log(\hat{\beta}_1*\hat{\beta}_2)$ is (`r quantile(boostrap_results$logbeta, 0.025, na.rm = TRUE)`, `r quantile(boostrap_results$logbeta, 0.975, na.rm = TRUE)`)

# Problem 3

## Import and clean data

```{r}
birthwt_df =
  read_csv("./data/birthweight.csv") |>
  mutate(
    babysex = factor(babysex, levels = c("1", "2"), labels = c("male", "female")),
    frace = factor(frace, levels = c("1", "2", "3", "4", "8", "9"),
                      labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    malform = factor(malform, levels = c("0", "1"), labels = c("absent", "present")),
    mrace = factor(mrace, levels = c("1", "2", "3", "4", "8"), 
                      labels = c("White", "Black", "Asian", "Puerto Rican", "Other"))
  )
```

## My own model

Intuitively, it's reasonable that the mother's physique and race are related to birth weight of the baby. So I choose mheight, mrace and ppbmi as predictors. This model is baesd on a hypothesized structure for the factors that underly birthweight.

```{r}
my_model = lm(bwt ~ mheight + mrace + ppbmi, data = birthwt_df)

my_model |> 
  broom::tidy() |> 
  select(term, estimate, p.value) |> 
  knitr::kable(digits = 5)
```

The most p.value of terms in this model are significant, suggesting my model is reasonable.

Add residuals and fitted values, make plot.

```{r}
birthwt_df |>
  modelr::add_residuals(my_model) |>
  modelr::add_predictions(my_model) |>
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() +
  ggtitle("Point Plot for Model Residuals against Fitted Values")
```

## Compare three models

Fit three models and calculate rmse.

```{r}
cv_df = 
  crossv_mc(birthwt_df, 100) 
cv_df = 
  cv_df |> 
  mutate(
    my_mod  = map(train, \(df) lm(
      bwt ~ mheight + mrace + ppbmi, data = df)),
    main_mod  = map(train, \(df) lm(
      bwt ~ blength + gaweeks, data = df)),
    interaction_mod  = map(train, \(df) lm(
      bwt ~ bhead*blength*babysex, data = df))
    ) |> 
  mutate(
    rmse_my = map2_dbl(my_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_main = map2_dbl(main_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_interaction = map2_dbl(interaction_mod, test, \(mod, df) rmse(model = mod, data = df)))
```

Make violin plot

```{r}
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  ggtitle("Comparison of RMSE among Three Different Models")
```

According to the violin plot, the cross-validated rmse of main effects model and interaction model are obviously smaller than my own model. Between these two models, interaction model's performance is even better, with lower rmse and more concentrated rmse distribution. So the interaction model is the optimal among all three models.
